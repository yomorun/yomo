package ai

import (
	"errors"
	"fmt"

	"github.com/yomorun/yomo/core/ylog"
)

// ChatCompletionRequest represents a request structure for chat completion API
type ChatCompletionRequest struct {
	Model    string                  `json:"model"`
	Messages []ChatCompletionMessage `json:"messages"`
	// MaxTokens 	the maximum number of tokens that can be generated in the chat completion
	MaxTokens int `json:"max_tokens,omitempty"`
	// Temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic
	Temperature float32 `json:"temperature,omitempty"`
	// TopP an alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered
	TopP float32 `json:"top_p,omitempty"`
	// N how many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs
	N int `json:"n,omitempty"`
	// Stream  if set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message
	Stream bool `json:"stream,omitempty"`
	// Stop up to 4 sequences where the API will stop generating further tokens
	Stop []string `json:"stop,omitempty"`
	// PresencePenalty number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics
	PresencePenalty float32 `json:"presence_penalty,omitempty"`
	// ResponseFormat represents the response format
	ResponseFormat *ChatCompletionResponseFormat `json:"response_format,omitempty"`
	// Seed            *int                          `json:"seed,omitempty"`
	// FrequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim
	FrequencyPenalty float32 `json:"frequency_penalty,omitempty"`
	// LogitBias is the likelihood of specified tokens appearing in the completion
	LogitBias map[string]int `json:"logit_bias,omitempty"`
	// LogProbs whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model
	LogProbs bool `json:"logprobs,omitempty"`
	// TopLogProbs an integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used
	TopLogProbs int    `json:"top_logprobs,omitempty"`
	User        string `json:"user,omitempty"`
	// Tools describes the tool calls generated by the model
	Tools []ToolCall `json:"tools,omitempty"`
	// This can be either a string or an ToolChoice object
	ToolChoice any `json:"tool_choice,omitempty"`
}

// ChatCompletionMessage represents a message structure for chat completion API
type ChatCompletionMessage struct {
	// Role is the messages author
	Role string `json:"role"`
	// Content of the message
	Content string `json:"content"`
	// Name describes participant, provides the model information to differentiate
	Name string `json:"name,omitempty"`
	// ToolCalls describes the tool calls generated by the model
	ToolCalls []ToolCall `json:"tool_calls,omitempty"`
	// ToolCallID is the ID of the tool call
	ToolCallID string `json:"tool_call_id,omitempty"`
}

// ChatCompletionResponseFormat represents the response format for chat completion API
type ChatCompletionResponseFormat struct {
	Type string `json:"type,omitempty"`
}

// ChatCompletionResponseFormat represents the response format for chat completion API
type ChatCompletionResponse struct {
	// ID is the unique identifier for the chat completion
	ID string `json:"id"`
	// Object describes the object type, it is always "chat.completion"
	Object string `json:"object"`
	// Created describes the timestamp when the chat completion was created
	Created int `json:"created"`
	// Model describes the model used for the chat completion
	Model string `json:"model"`
	// Choices describes the choices made by the model, can more than one if `n`>1
	Choices []ChatCompletionChoice `json:"choices"`
	// Usage describes the token usage statistics for the chat completion request
	Usage Usage `json:"usage"`
	// SystemFingerprint describes the system fingerprint of the chat completion
	SystemFingerprint string `json:"system_fingerprint"`
}

// ChatCompletionChoice represents the choice in chat completion API
type ChatCompletionChoice struct {
	Index        int                   `json:"index"`
	Message      ChatCompletionMessage `json:"message"`
	FinishReason string                `json:"finish_reason"`
	// LogProbs     *LogProbs    `json:"logprobs,omitempty"`
}

// Usage Represents the total token usage per request to OpenAI
type Usage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

func (res *ChatCompletionResponse) ConvertToInvokeResponse(tcs map[uint32]ToolCall) (*InvokeResponse, error) {
	choice := res.Choices[0]
	ylog.Debug(">>finish_reason", "reason", choice.FinishReason)
	responseMessage := res.Choices[0].Message
	calls := responseMessage.ToolCalls
	ylog.Debug("--response calls", "calls", len(calls))
	content := responseMessage.Content

	result := &InvokeResponse{}
	// finish reason
	result.FinishReason = choice.FinishReason
	result.Content = content

	// record usage
	result.TokenUsage = TokenUsage{
		PromptTokens:     res.Usage.PromptTokens,
		CompletionTokens: res.Usage.CompletionTokens,
	}
	ylog.Debug("++ llm result", "token_usage", fmt.Sprintf("%v", result.TokenUsage), "finish_reason", result.FinishReason)

	// if llm said no function call, we should return the result
	if result.FinishReason == "stop" {
		return result, nil
	}

	if result.FinishReason == "tool_calls" {
		// assistant message
		result.AssistantMessage = responseMessage
	}

	if len(calls) == 0 {
		return result, errors.New("finish_reason is tool_calls, but no tool calls found")
	}

	// functions may be more than one
	for _, call := range calls {
		for tag, tc := range tcs {
			if tc.Equal(call) {
				// use toolCalls because tool_id is required in the following llm request
				if result.ToolCalls == nil {
					result.ToolCalls = make(map[uint32][]*ToolCall)
				}
				// create a new variable to hold the current call
				currentCall := call
				result.ToolCalls[tag] = append(result.ToolCalls[tag], &currentCall)
			}
		}
	}

	return result, nil
}
