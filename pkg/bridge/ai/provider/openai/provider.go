// Package openai is the OpenAI llm provider
package openai

import (
	"fmt"
	"os"

	// automatically load .env file
	_ "github.com/joho/godotenv/autoload"
	"github.com/yomorun/yomo/ai"
	"github.com/yomorun/yomo/core/metadata"
	"github.com/yomorun/yomo/core/ylog"
	"github.com/yomorun/yomo/pkg/bridge/ai/internal/openai"
)

// APIEndpoint is the endpoint for OpenAI
const APIEndpoint = "https://api.openai.com/v1/chat/completions"

// // ChatCompletionMessage describes `messages` for /chat/completions
// type ChatCompletionMessage struct {
// 	// Role is the messages author
// 	Role string `json:"role"`
// 	// Content of the message
// 	Content string `json:"content"`
// 	// Name describes participant, provides the model information to differentiate
// 	// between participants of the same role.
// 	Name string `json:"name,omitempty"`
// 	// ToolCalls describes the tool calls generated by the model.
// 	ToolCalls []ai.ToolCall `json:"tool_calls,omitempty"`
// 	// ToolCallID is the ID of the tool call
// 	ToolCallID string `json:"tool_call_id,omitempty"`
// }

// // ReqBody is the request body
// type ReqBody struct {
// 	// Model describes the ID of the model to use for the completion.
// 	Model string `json:"model"`
// 	// Messages describes the messages in the conversation.
// 	Messages []ChatCompletionMessage `json:"messages"`
// 	// Tools describes the tool calls generated by the model.
// 	Tools []ai.ToolCall `json:"tools"` // chatCompletionTool
// }

// // RespBody is the response body
// type RespBody struct {
// 	// ID is the unique identifier for the chat completion.
// 	ID string `json:"id"`
// 	// Object describes the object type, it is always "chat.completion".
// 	Object string `json:"object"`
// 	// Created describes the timestamp when the chat completion was created.
// 	Created int `json:"created"`
// 	// Model describes the model used for the chat completion.
// 	Model string `json:"model"`
// 	// Choices describes the choices made by the model, can more than one if `n`>1
// 	Choices []RespChoice `json:"choices"`
// 	// Usage describes the token usage statistics for the chat completion request.
// 	Usage openai.RespUsage `json:"usage"`
// 	// SystemFingerprint describes the system fingerprint of the chat completion.
// 	SystemFingerprint string `json:"system_fingerprint"`
// }

// // RespMessage is the message in Response
// type RespMessage struct {
// 	Role      string        `json:"role"`
// 	Content   string        `json:"content"`
// 	ToolCalls []ai.ToolCall `json:"tool_calls"`
// }

// // RespChoice is used to indicate the choice in Response by `FinishReason`
// type RespChoice struct {
// 	FinishReason string                `json:"finish_reason"`
// 	Index        int                   `json:"index"`
// 	Message      ChatCompletionMessage `json:"message"`
// }

// // RespUsage is the token usage in Response
// type RespUsage struct {
// 	PromptTokens     int `json:"prompt_tokens"`
// 	CompletionTokens int `json:"completion_tokens"`
// 	TotalTokens      int `json:"total_tokens"`
// }

// OpenAIProvider is the provider for OpenAI
type OpenAIProvider struct {
	// APIKey is the API key for OpenAI
	APIKey string
	// Model is the model for OpenAI
	// eg. "gpt-3.5-turbo-1106", "gpt-4-turbo-preview", "gpt-4-vision-preview", "gpt-4"
	Model string
}

// NewProvider creates a new OpenAIProvider
func NewProvider(apiKey string, model string) *OpenAIProvider {
	if apiKey == "" {
		apiKey = os.Getenv("OPENAI_API_KEY")
	}
	if model == "" {
		model = os.Getenv("OPENAI_MODEL")
	}
	ylog.Debug("new openai provider", "api_endpoint", APIEndpoint, "api_key", apiKey, "model", model)
	return &OpenAIProvider{
		APIKey: apiKey,
		Model:  model,
	}
}

// Name returns the name of the provider
func (p *OpenAIProvider) Name() string {
	return "openai"
}

// GetChatCompletions get chat completions for ai service
func (p *OpenAIProvider) GetChatCompletions(userInstruction string, md metadata.M) (*ai.InvokeResponse, error) {
	// messages
	userDefinedBaseSystemMessage := `You are a very helpful assistant. Your job is to choose the best possible action to solve the user question or task. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.`

	reqBody := openai.ReqBody{Model: p.Model}

	res, err := openai.ChatCompletion(APIEndpoint, "Authorization", fmt.Sprintf("Bearer %s", p.APIKey), reqBody, userDefinedBaseSystemMessage, userInstruction, nil, md)
	if err != nil {
		return nil, err
	}

	return res, nil
}

// // GetChatCompletions get chat completions for ai service
// func (p *OpenAIProvider) GetChatCompletions1(userInstruction string, md metadata.M) (*ai.InvokeResponse, error) {
// 	tcs, err := register.ListToolCalls(md)
// 	if err != nil {
// 		return nil, err
// 	}
// 	if len(tcs) == 0 {
// 		ylog.Error(ai.ErrNoFunctionCall.Error())
// 		return &ai.InvokeResponse{Content: "no toolcalls"}, ai.ErrNoFunctionCall
// 	}

// 	// messages
// 	messages := []openai.ChatCompletionMessage{
// 		{Role: "system", Content: `You are a very helpful assistant. Your job is to choose the best possible action to solve the user question or task. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. If you don't know the answer, stop the conversation by saying "no func call".`},
// 		{Role: "user", Content: userInstruction},
// 	}

// 	// prepare tools
// 	toolCalls := make([]ai.ToolCall, len(tcs))
// 	idx := 0
// 	for _, tc := range tcs {
// 		toolCalls[idx] = tc
// 		idx++
// 	}

// 	body := openai.ReqBody{Model: p.Model, Messages: messages, Tools: toolCalls}
// 	ylog.Debug("request", "tools", len(toolCalls), "messages", messages)

// 	jsonBody, err := json.Marshal(body)
// 	if err != nil {
// 		return nil, err
// 	}

// 	req, err := http.NewRequest("POST", APIEndpoint, bytes.NewBuffer(jsonBody))
// 	if err != nil {
// 		return nil, err
// 	}
// 	req.Header.Set("Content-Type", "application/json")
// 	// OpenAI authentication
// 	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", p.APIKey))

// 	client := &http.Client{}
// 	resp, err := client.Do(req)
// 	if err != nil {
// 		return nil, err
// 	}
// 	defer resp.Body.Close()

// 	respBody, err := io.ReadAll(resp.Body)
// 	if err != nil {
// 		return nil, err
// 	}
// 	ylog.Debug("response", "body", respBody)

// 	// ylog.Info("response body", "body", string(respBody))
// 	if resp.StatusCode >= 400 {
// 		return nil, fmt.Errorf("ai response status code is %d", resp.StatusCode)
// 	}

// 	var respBodyStruct openai.RespBody
// 	err = json.Unmarshal(respBody, &respBodyStruct)
// 	if err != nil {
// 		return nil, err
// 	}
// 	// TODO: record usage
// 	// usage := respBodyStruct.Usage
// 	// log.Printf("Token Usage: %+v\n", usage)

// 	choice := respBodyStruct.Choices[0]
// 	ylog.Debug(">>finish_reason", "reason", choice.FinishReason)

// 	calls := respBodyStruct.Choices[0].Message.ToolCalls
// 	content := respBodyStruct.Choices[0].Message.Content

// 	ylog.Debug("--response calls", "calls", calls)

// 	result := &ai.InvokeResponse{}
// 	if len(calls) == 0 {
// 		result.Content = content
// 		return result, ai.ErrNoFunctionCall
// 	}

// 	// functions may be more than one
// 	// slog.Info("tool calls", "calls", calls, "mapTools", mapTools)
// 	for _, call := range calls {
// 		for tag, tc := range tcs {
// 			if tc.Equal(&call) {
// 				// Use toolCalls because tool_id is required in the following llm request
// 				if result.ToolCalls == nil {
// 					result.ToolCalls = make(map[uint32][]*ai.ToolCall)
// 				}
// 				// Create a new variable to hold the current call
// 				currentCall := call
// 				result.ToolCalls[tag] = append(result.ToolCalls[tag], &currentCall)
// 			}
// 		}
// 	}

// 	// sfn maybe disconnected, so we need to check if there is any function call
// 	if len(result.ToolCalls) == 0 {
// 		return nil, ai.ErrNoFunctionCall
// 	}
// 	return result, nil
// }
